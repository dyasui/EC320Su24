---
name: assumptionsA1-A3
---

---

## Residuals *vs.* Errors

The most important assumptions concern the error term $u_i$.

. . .

[Important:]{.hi} An error $u_i$ and a residual $\hat{u}_i$ are related,
but different.

. . .

[Error:]{.hi-green} 

> Difference between the wage of a worker with 16 years of education and the [_expected wage_]{.hi-green} with 16 years of education.


. . .

[Residual:]{.hii}

> Difference between the wage of a worker with 16 years of education and the [_average wage_]{.hii} of workers with 16 years of education.

. . .

:::{.align-center}
[Population]{.hi-green} [_vs._]{.note} [sample]{.hii}
:::

---

## Residuals *vs.* Errors

A [residual]{.hii} tells us how a [worker]{.hi}'s wages compare to the average wages of workers in the [sample]{.hii} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

# Neumark data
wage <- get(data(wage2))
lm_e <- lm(lwage ~ educ, data = wage)
b0 <- lm_e$coefficients[1]
b1 <- lm_e$coefficients[2]
r_2 <- summary(lm(lwage ~ educ, data = wage))$r.squared

y_hat <- function(x, b0, b1) {b0 + b1 * x}
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = hii, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Residuals *vs.* Errors

A [residual]{.hii} tells us how a [worker]{.hi}'s wages compare to the average wages of workers in the [sample]{.hii} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

y_hat <- function(x, b0, b1) {b0 + b1 * x}
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0.1) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = hii, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Residuals *vs.* Errors

An [error]{.hi-green} tells us how a [worker]{.hi}'s wages compare to the expected wages of workers in the [population]{.hi-green} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

y_hat <- function(x, b0, b1) {b0 + b1 * x}
B0 <- b0 + 0.5
B1 <- b1 - 0.035
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, B0, B1), color = (7.75 - y_hat(11, B0, B1))^2), linetype = "solid", color = higreen, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  geom_abline(intercept = B0, slope = B1, color = higreen, size = 1, linetype = "solid") +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Classical Assumptions of OLS

. . .

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

---

## Linearity ([A1.]{.note})

> The population relationship is [_linear in parameters_]{.note} with an additive error term.

. . .

[Examples]{.hi}

- $\text{Wage}_i = \beta_1 + \beta_2 \text{Experience}_i + u_i$

. . .

- $\log(\text{Happiness}_i) = \beta_1 + \beta_2 \log(\text{Money}_i) + u_i$

. . .

- $\sqrt{\text{Convictions}_i} = \beta_1 + \beta_2 (\text{Early Childhood Lead Exposure})_i + u_i$

. . .

- $\log(\text{Earnings}_i) = \beta_1 + \beta_2 \text{Education}_i + u_i$

---

## Linearity ([A1.]{.note})

> The population relationship is [_linear in parameters_]{.note} with an additive error term.

[Violations]{.hi}

- $\text{Wage}_i = (\beta_1 + \beta_2 \text{Experience}_i)u_i$

. . .

- $\text{Consumption}_i = \frac{1}{\beta_1 + \beta_2 \text{Income}_i} + u_i$

. . .

- $\text{Population}_i = \frac{\beta_1}{1 + e^{\beta_2 + \beta_3 \text{Food}_i}} + u_i$

. . .

- $\text{Batting Average}_i = \beta_1 (\text{Wheaties Consumption})_i^{\beta_2} + u_i$

---

## Classical Assumptions of OLS {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

. . .

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.


---

## Sample Variation ([A2.]{.note})

> There is variation in $X$.

[Example]{.hi}

```{r}
#| echo: false
#| fig.height: 4
#| fig.align: center

wage %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = hi) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Sample Variation ([A2.]{.note})

> There is variation in $X$.

[Violation]{.hi}

```{r}
#| echo: false
#| fig.height: 4
#| fig.align: center

wage %>% 
  filter(educ == 13) %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = hi) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## {data-visibility="uncounted"}

::: {.vertical-center}
As we will see later, [sample variation]{.note} matters for inference as well.
:::

---

## Classical Assumptions of OLS {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

. . .

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 


---

## Exogeniety ([A3.]{.note})

Assumption

> The $X$ variable is [exogenous:]{.note} 

$$
\mathop{\mathbb{E}}\left( u|X \right) = 0
$$

. . .

The assignment of $X$ is effectively random.

<br>

. . .

_Implication:_ no [selection bias]{.hp} or [omitted variable bias]{.hii}

---

## Exogeniety ([A3.]{.note}) {data-visibility="uncounted"}

Assumption

> The $X$ variable is [exogenous:]{.note} 

$$
\mathop{\mathbb{E}}\left( u|X \right) = 0
$$

[Example]{.hi}

In the labor market, an important component of $u$ is unobserved ability.

- $\mathop{\mathbb{E}}\left( u|\text{Education} = 12 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Education} = 20 \right) = 0$.
- $\mathop{\mathbb{E}}\left( u|\text{Experience} = 0 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Experience} = 40 \right) = 0$.
- _Do you believe this?_

---

## {data-visibility="uncounted"}

:::{.vertical-center}
Graphically...
:::

---

## 

```{r}
#| include: false
#| cache: true

# Setup ----------------------------------------------------------------------------------
  # Packages
  library(pacman)
  p_load(ggridges)
# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n) + 0.5 * (x - 13.5),
  ) %>% mutate(x = round(x))
# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    mytheme
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    mytheme
# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    mytheme +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = hi),
      axis.title.x = element_text(size = 22, color = hi)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    mytheme +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = hi),
      axis.title.x = element_text(size = 22, color = hi)
    )
```

Valid exogeniety, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) = 0$

```{r}
#| echo: false
#| fig.height: 6
#| fig.align: center

cond_good
```

---

##

Invalid exogeniety, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) \neq 0$

```{r}
#| echo: false
#| fig.height: 6
#| fig.align: center

cond_bad
```

---

##

:::{.vertical-center}
[_When can we trust OLS?_]{.note}
:::

---

## Bias

An estimator is [biased]{.hi-red} if its expected value is different from the true population parameter.

:::: {.columns}

::: {.column width="50%"}
[Unbiased estimator:]{.hi} $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] = \beta$

```{r}
#| echo: false
#| fig.height: 5.75
#| fig.align: center

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = hp, alpha = 0.9) +
geom_hline(yintercept = 0, color = hi) +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = "ÃŸ") + 
mytheme_s 

# +
# theme(axis.text.x = element_text(size = 40),
#       axis.text.y = element_blank(),
#       axis.title = element_blank(),
#       line = element_blank())
```

:::

::: {.column width="50%"}
[Biased estimator:]{.hi-red} $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] \neq \beta$

```{r}
#| echo: false
#| fig.height: 5.75
#| fig.align: center

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = hi, alpha = 0.9) +
geom_hline(yintercept = 0, color = hi) +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = "ÃŸ") +
mytheme_s 

# +
# theme(axis.text.x = element_text(size = 40),
#       axis.text.y = element_blank(),
#       axis.title = element_blank(),
#       line = element_blank())
```

:::

::::


---



## 

::: {.vertical-center}
[Let's prove unbiasedness of OLS]{.note}
:::

---

[Proving unbiasedness of simple OLS]{.note}

Suppose we have the following model

$$
Y_i = \beta_0 + \beta_1 X_i + u_i
$$

. . .

The slope parameter follows as:

$$
\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}
$$

. . .

(_As shown in section 2.3 in ItE_) that the estimator $\hat{\beta_1}$, can be broken up into a nonrandom and a random component:

---

<!-- [Proving unbiasedness of simple OLS]{.note} -->

Substitute for $y_i$:

$$
\hat{\beta}_1 = \frac{\sum((\beta_0 + \beta_1x_i + u_i) - \bar{y})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

. . .

Substitute $\bar{y} = \beta_0 + \beta_1\bar{x}$:

$$
 = \frac{\sum(\beta_0 + \beta_1x_i + u_i - \beta_0 - \beta_1\bar{x} - \bar{u})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

. . .

$$
= \frac{\sum[\beta_1 (x_i - \bar{x}) + (u_i - \bar{u})](x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

. . .

$$
= \frac{ \sum\beta_1(x_i - \bar{x})(x_i-\bar{x}) + \sum(u_i - \bar{u})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

---

Now we can apply the summation rules to factor out $\beta_1$:
$$
\hat{\beta}_1 = \beta_1 \frac{\sum(x_i - \bar{x})^2}{\sum(x_i - \bar{x})^2} + \frac{\sum(u_i - \bar{u})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

. . .

Now we have the non-random component, $\beta_1$, by itself:

$$
\hat{\beta}_1 = \beta_1 + \frac{\sum(u_i - \bar{u})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

- I.e., a nonrandom $\beta_1$ which is our target *estimand*

- and an additional random term: $\frac{\sum(u_i - \bar{u})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}$

---

A few more steps to make our proof easier later:

- Let's take the denominator of the random fraction term in the previous equation:

$$
\begin{align}
  \sum(u_i - \hat{u})(x_i - \bar{x}) & = \sum u_i (x_i - \bar{x}) + \bar{u} \sum (x_i - \bar{x}) \\
  & = \sum u_i (x_i - \bar{x}) + \bar{u} (\sum x_i - n \bar{X}) \\
  & = \sum (x_i - \bar{x}) u_i
\end{align}
$$

- Let's also define a new variable: $w_i$ for *weights*

$$
w_i = \frac{(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

---

[Proving unbiasedness of simple OLS]{.note}

Now we can rewrite our estimator a bit more neatly:
$$
\hat{\beta}_1 = \beta_1 + \sum{w_i u_i}
$$

Remember what unbiasedness means as our goal here:

- We want our estimate $\hat{\beta_1}$ to be as close to the true $\beta_1$ 
as possible.

. . .

- If the estimator is [unbiased]{.hi}, then
$$
\mathbb{E}[\hat{\beta_1}] = \beta_1
$$

---

$$
\hat{\beta}_1 = \beta_1 + \sum{w_i u_i}
$$

. . .

Taking the expectation, 

$$
\mathbb{E}[\hat{\beta_1}] = \mathbb{E}[\beta_1] + \sum \mathbb{E}[ w_i u_i]
$$

. . .

What to do from here? 

- the weights are still random variables, so we can't simplify by pulling them out of the expected value

. . . 

- But what if we could treat them as constants?

---

[Proving unbiasedness of simple OLS]{.note}

::: {.callout-note}
If the [conditional expectation]{.hi} of something is a constant,
then the *unconditional expectation* will equal that same constant.
:::

. . .

- for example, consider IQ:
  - if IQ's expected value is 100 at every age,
  then the expected value will be 100 in the overall population as well!

---

[Proving unbiasedness of simple OLS]{.note}

How can we apply the conditional expectaion rule to our proof?

. . .

If $\mathbb{E}[w_iu_i|x_i] =$ some constant $c$, then we know $\mathbb{E}[w_iu_i] = c$ as well.

. . .

Why does this help? 
- Recall that $w_i$ is just composed of $x_i$ and some constants, so:

$$
\mathbb{E}[w_iu_i|x_i] = w_i \mathbb{E}[u_i|x_i]
$$

. . .

- And what constant is $\mathbb{E}[u_i|x_i]$ equal to?

. . .

- [A3]{.hii} $\Rightarrow \mathbb{E}[u_i|x_i] = 0$!!!

---

[Proving unbiasedness of simple OLS]{.note}

Putting it all together:

. . .

  - we can decompose the expected value of $\beta_1$:
$$
\mathbb{E}[\hat{\beta_1}] = \beta_1 + \sum \mathbb{E}[ w_i u_i]
$$

. . .

  - If [A3]{.hii} is true, then we can apply the conditional expectation rule to show that:

$$
\mathbb{E}[\hat{\beta_1}] = \beta_1 + 0
$$

. . .

***So OLS is unbiased !!!***

## Required Assumptions {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 

[Result:]{.hi} [OLS is unbiased.]{.fragment}
