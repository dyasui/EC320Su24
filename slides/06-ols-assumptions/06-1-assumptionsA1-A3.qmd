---
name: assumptionsA1-A3
---

---

## Residuals *vs.* Errors

The most important assumptions concern the error term $u_i$.

. . .

[Important:]{.hi} An error $u_i$ and a residual $\hat{u}_i$ are related,
but different.

. . .

[Error:]{.hi-green} 

> Difference between the wage of a worker with 16 years of education and the [_expected wage_]{.hi-green} with 16 years of education.


. . .

[Residual:]{.hii}

> Difference between the wage of a worker with 16 years of education and the [_average wage_]{.hii} of workers with 16 years of education.

. . .

:::{.align-center}
[Population]{.hi-green} [_vs._]{.note} [sample]{.hii}
:::

---

## Residuals *vs.* Errors

A [residual]{.hii} tells us how a [worker]{.hi}'s wages compare to the average wages of workers in the [sample]{.hii} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

# Neumark data
wage <- get(data(wage2))
lm_e <- lm(lwage ~ educ, data = wage)
b0 <- lm_e$coefficients[1]
b1 <- lm_e$coefficients[2]
r_2 <- summary(lm(lwage ~ educ, data = wage))$r.squared

y_hat <- function(x, b0, b1) {b0 + b1 * x}
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = hii, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Residuals *vs.* Errors

A [residual]{.hii} tells us how a [worker]{.hi}'s wages compare to the average wages of workers in the [sample]{.hii} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

y_hat <- function(x, b0, b1) {b0 + b1 * x}
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0.1) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, b0, b1), color = (7.75 - y_hat(11, b0, b1))^2), linetype = "solid", color = hii, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Residuals *vs.* Errors

An [error]{.hi-green} tells us how a [worker]{.hi}'s wages compare to the expected wages of workers in the [population]{.hi-green} with the same level of education.

```{r}
#| echo: false
#| fig.height: 5.5
#| fig.align: center

y_hat <- function(x, b0, b1) {b0 + b1 * x}
B0 <- b0 + 0.5
B1 <- b1 - 0.035
wage %>% 
  ggplot(aes(x = educ, y = lwage)) +
  geom_point(color = hi, alpha = 0) +
  geom_segment(aes(x = 11, xend = 11, y = 7.75, yend = y_hat(11, B0, B1), color = (7.75 - y_hat(11, B0, B1))^2), linetype = "solid", color = higreen, size = 0.5) +
  geom_point(aes(x = 11, y = 7.75), color = hi, size = 3) +
  geom_abline(intercept = b0, slope = b1, color = hp, size = 1) +
  geom_abline(intercept = B0, slope = B1, color = higreen, size = 1, linetype = "solid") +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Classical Assumptions of OLS

. . .

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

---

## Linearity ([A1.]{.note})

> The population relationship is [_linear in parameters_]{.note} with an additive error term.

. . .

[Examples]{.hi}

- $\text{Wage}_i = \beta_1 + \beta_2 \text{Experience}_i + u_i$

. . .

- $\log(\text{Happiness}_i) = \beta_1 + \beta_2 \log(\text{Money}_i) + u_i$

. . .

- $\sqrt{\text{Convictions}_i} = \beta_1 + \beta_2 (\text{Early Childhood Lead Exposure})_i + u_i$

. . .

- $\log(\text{Earnings}_i) = \beta_1 + \beta_2 \text{Education}_i + u_i$

---

## Linearity ([A1.]{.note})

> The population relationship is [_linear in parameters_]{.note} with an additive error term.

[Violations]{.hi}

- $\text{Wage}_i = (\beta_1 + \beta_2 \text{Experience}_i)u_i$

. . .

- $\text{Consumption}_i = \frac{1}{\beta_1 + \beta_2 \text{Income}_i} + u_i$

. . .

- $\text{Population}_i = \frac{\beta_1}{1 + e^{\beta_2 + \beta_3 \text{Food}_i}} + u_i$

. . .

- $\text{Batting Average}_i = \beta_1 (\text{Wheaties Consumption})_i^{\beta_2} + u_i$

---

## Classical Assumptions of OLS {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

. . .

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.


---

## Sample Variation ([A2.]{.note})

> There is variation in $X$.

[Example]{.hi}

```{r}
#| echo: false
#| fig.height: 4
#| fig.align: center

wage %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = hi) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## Sample Variation ([A2.]{.note})

> There is variation in $X$.

[Violation]{.hi}

```{r}
#| echo: false
#| fig.height: 4
#| fig.align: center

wage %>% 
  filter(educ == 13) %>% 
  ggplot() +
  xlim(9, 18) + ylim(4.5, 8.1) +
  geom_point(aes(x = educ, y = lwage), color = hi) +
  mytheme_s + xlab("Years of Education") + ylab("log(Monthly Earnings)")
```

---

## {data-visibility="uncounted"}

::: {.vertical-center}
As we will see later, [sample variation]{.note} matters for inference as well.
:::

---

## Classical Assumptions of OLS {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

. . .

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 


---

## Exogeniety ([A3.]{.note})

Assumption

> The $X$ variable is [exogenous:]{.note} 

$$
\mathop{\mathbb{E}}\left( u|X \right) = 0
$$

. . .

The assignment of $X$ is effectively random.

<br>

. . .

_Implication:_ no [selection bias]{.hp} or [omitted variable bias]{.hii}

---

## Exogeniety ([A3.]{.note}) {data-visibility="uncounted"}

Assumption

> The $X$ variable is [exogenous:]{.note} 

$$
\mathop{\mathbb{E}}\left( u|X \right) = 0
$$

[Example]{.hi}

In the labor market, an important component of $u$ is unobserved ability.

- $\mathop{\mathbb{E}}\left( u|\text{Education} = 12 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Education} = 20 \right) = 0$.
- $\mathop{\mathbb{E}}\left( u|\text{Experience} = 0 \right) = 0$ and $\mathop{\mathbb{E}}\left( u|\text{Experience} = 40 \right) = 0$.
- _Do you believe this?_

---

## {data-visibility="uncounted"}

:::{.vertical-center}
Graphically...
:::

---

## 

```{r}
#| include: false
#| cache: true

# Setup ----------------------------------------------------------------------------------
  # Packages
  library(pacman)
  p_load(ggridges)
# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = 9, max = 18),
    e = rnorm(n) + 0.5 * (x - 13.5),
  ) %>% mutate(x = round(x))
# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    mytheme
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    mytheme
# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    mytheme +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = hi),
      axis.title.x = element_text(size = 22, color = hi)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("Unobserved Ability") +
    ylab("Years of Education") +
    mytheme +
    theme(
      legend.position = "none",
      axis.title.y = element_text(vjust = 0.5, size = 22, color = hi),
      axis.title.x = element_text(size = 22, color = hi)
    )
```

Valid exogeniety, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) = 0$

```{r}
#| echo: false
#| fig.height: 6
#| fig.align: center

cond_good
```

---

##

Invalid exogeniety, _i.e._, $\mathop{\mathbb{E}}\left( u \mid X \right) \neq 0$

```{r}
#| echo: false
#| fig.height: 6
#| fig.align: center

cond_bad
```

---

##

:::{.vertical-center}
[_When can we trust OLS?_]{.note}
:::

---

## Bias

An estimator is [biased]{.hi-red} if its expected value is different from the true population parameter.

:::: {.columns}

::: {.column width="50%"}
[Unbiased estimator:]{.hi} $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] = \beta$

```{r}
#| echo: false
#| fig.height: 5.75
#| fig.align: center

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = hp, alpha = 0.9) +
geom_hline(yintercept = 0, color = hi) +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = "ÃŸ") + 
mytheme_s 

# +
# theme(axis.text.x = element_text(size = 40),
#       axis.text.y = element_blank(),
#       axis.title = element_blank(),
#       line = element_blank())
```

:::

::: {.column width="50%"}
[Biased estimator:]{.hi-red} $\mathop{\mathbb{E}}\left[ \hat{\beta} \right] \neq \beta$

```{r}
#| echo: false
#| fig.height: 5.75
#| fig.align: center

tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = hi, alpha = 0.9) +
geom_hline(yintercept = 0, color = hi) +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = "ÃŸ") +
mytheme_s 

# +
# theme(axis.text.x = element_text(size = 40),
#       axis.text.y = element_blank(),
#       axis.title = element_blank(),
#       line = element_blank())
```

:::

::::


---

## Required Assumptions

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 

. . .

&#9755; (3) implies [random sampling]{.note}. [Without it, our [external validity]{.note} becomes uncertain.^[[Internal Validity:]{.note} Relates to how well a study is conducted (does it satisfy OLS assumptions?). <br> [External Validity:]{.note} Relates to how applicable the findings are to the real world.]]{.fragment}

---

## 

::: {.vertical-center}
[Let's prove unbiasedness of OLS]{.note}
:::

---

[Proving unbiasedness of simple OLS]{.note}

Suppose we have the following model

$$
Y_i = \beta_1 + \beta_2 X_i + u_i
$$

. . .

The slope parameter follows as:

$$
\hat{\beta}_2 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}
$$

. . .

(_As shown in section 2.3 in ItE_) that the estimator $\hat{\beta_2}$, can be broken up into a nonrandom and a random component:

---

[Proving unbiasedness of simple OLS]{.note}

Substitute for $y_i$:

$$
\hat{\beta}_2 = \frac{\sum((\beta_1 + \beta_2x_i + u_i) - \bar{y})(x_i - \bar{x})}{\sum(x_i - \bar{x})^2}
$$

. . .

Substitute $\bar{y} = \beta_1 + \beta_2\bar{x}$:

$$
\hat{\beta}_2 = \frac{\sum(u_i(x_i - \bar{x}))}{\sum(x_i - \bar{x})^2} + \frac{\sum(\beta_2x_i(x_i - \bar{x}))}{\sum(x_i - \bar{x})^2}
$$

. . .

The non-random component, $\beta_2$, is factored out:

$$
\hat{\beta}_2 = \frac{\sum(u_i(x_i - \bar{x}))}{\sum(x_i - \bar{x})^2} + \beta_2\frac{\sum(x_i(x_i - \bar{x}))}{\sum(x_i - \bar{x})^2}
$$

---

[Proving unbiasedness of simple OLS]{.note}

[Observe]{.note} that the second term is equal to 1. Thus, we have:

$$
\hat{\beta}_2 = \beta_2 + \frac{\sum(u_i(x_i - \bar{x}))}{\sum(x_i - \bar{x})^2}
$$

. . .

Taking the expectation, 

$$
\mathbb{E}[\hat{\beta_2}] = \mathbb{E}[\beta] + \mathbb{E} \left[\frac{\sum \hat{u_i} (x_i - \bar{x})}{\sum(x_i - \bar{x})^2} \right]
$$

. . .

By [Rules 01]{.hi} and [02]{.hi} of expected value and [A3]{.note}:

$$
\begin{equation*}
  \mathbb{E}[\hat{\beta_2}] = \beta + \frac{\sum \mathbb{E}[\hat{u_i}] (x_i - \bar{x})}{\sum(x_i - \bar{x})^2} = \beta
\end{equation*}
$$

---

## Required Assumptions {data-visibility="uncounted"}

[A1.]{.note} [Linearity:]{.hi} The population relationship is [_linear in parameters_]{.note} with an additive error term.

[A2.]{.note} [Sample Variation:]{.hi} There is variation in $X$.

[A3.]{.note} [Exogeniety:]{.hi} The $X$ variable is [exogenous]{.note} 

&#9755; (3) implies [random sampling]{.note}. Without it, our [external validity]{.note} becomes uncertain.^[[Internal Validity:]{.note} Relates to how well a study is conducted (does it satisfy OLS assumptions?). <br> [External Validity:]{.note} Relates to how applicable the findings are to the real world.]

<br>

[Result:]{.hi} [OLS is unbiased.]{.fragment}


